using Distributions: DiscreteDistribution, ContinuousDistribution
using Flux: softmax, gradient, params
using NNlib: logsoftmax

export action_distribution, policy_gradient_estimate, IsPolicyGradient
export conjugate_gradient!

struct IsPolicyGradient end
IsPolicyGradient(::T) where T = IsPolicyGradient(T)
IsPolicyGradient(T::Type) = error("Type $T does not implement `IsPolicyGradient`")

"""
    action_distribution(dist, model_output)

Compute the action distribution using the distribution type and output from a model.
"""
action_distribution(dist, model_output) =
    throw(ArgumentError("dist ($dist) is not a ContinuousDistribution or DiscreteDistribution, not implemented"))

"""
    action_distribution(dist::Type{T}, model_output) where {T<:DiscreteDistribution}

When `dist` is a subtype of `DiscreteDistribution`, assume `model_output` is a batch of unnormalized log probabilities.
# Examples
```jldoctest
julia> model_output = reshape(1:10, 5, 2)
5×2 reshape(::UnitRange{Int64}, 5, 2) with eltype Int64:
 1   6
 2   7
 3   8
 4   9
 5  10
julia> action_distribution(Categorical, model_output)
2-element Vector{Categorical{Float64, Vector{Float64}}}:
 Categorical{Float64, Vector{Float64}}(
support: Base.OneTo(5)
p: [0.011656230956039605, 0.03168492079612427, 0.0861285444362687, 0.23412165725273662, 0.6364086465588308]
)

 Categorical{Float64, Vector{Float64}}(
support: Base.OneTo(5)
p: [0.011656230956039605, 0.03168492079612427, 0.0861285444362687, 0.23412165725273662, 0.6364086465588308]
)
```
"""
action_distribution(dist::Type{T}, model_output) where {T<:DiscreteDistribution} = 
    map(col -> dist(col; check_args=false), eachcol(softmax(model_output)))

"""
    action_distribution(dist::Type{T}, model_output) where {T<:ContinuousDistribution}

When `dist` is a subtype of `ContinuousDistribution`, assume `model_output` are a batch of parameters to be supplied to `dist`.
# Examples
```jldoctest
julia> model_output = reshape(1:10, 2, 5)
2×5 reshape(::UnitRange{Int64}, 2, 5) with eltype Int64:
 1  3  5  7   9
 2  4  6  8  10
julia> action_distribution(Normal, model_output)
5-element Vector{Normal{Float64}}:
 Normal{Float64}(μ=1.0, σ=2.0)
 Normal{Float64}(μ=3.0, σ=4.0)
 Normal{Float64}(μ=5.0, σ=6.0)
 Normal{Float64}(μ=7.0, σ=8.0)
 Normal{Float64}(μ=9.0, σ=10.0)
```
"""
action_distribution(dist::Type{T}, model_output) where {T<:ContinuousDistribution} = 
    map(col -> dist(col...), eachcol(model_output))

"""
    policy_gradient_estimate(policy::AbstractPolicy, states, actions, advantage)
Estimate the policy gradient from a batch of aligned states, actions, and advantages.
"""
policy_gradient_estimate(policy::AbstractPolicy, states, actions, advantage) =
    policy_gradient_estimate(IsPolicyGradient(policy), policy, states, actions, advantage)

function policy_gradient_estimate(::IsPolicyGradient, policy, states, actions, advantage)
    gs = gradient(params(policy.approximator)) do
        action_logits = action_distribution(policy.dist, policy.approximator(states))
        total_loss = logpdf.(action_logits, actions) .* advantage
        loss = -mean(total_loss)
        loss
    end
    gs
end

"""
    gvp(model, states, action_logits, x)

Calculate the gradient-vector product ``gx``,
where ``g`` is the gradient of the average Kullback–Leibler divergence
between the `model` output on a batch of `states` (taken as logits) and
action_logits, with regard to the model parameters.
"""
function gvp(model, states, action_logits, x)
    gs = Flux.gradient(Flux.params(model)) do 
        kld(model, states, action_logits)
    end
    mapreduce(vec, vcat, gs)' * x
end

"""
    hvp(model, states, action_logits, x; damping_coeff = 1f-2)

Calculate the Hessian-vector product ``Hx+damping_coeff\\cdot x``,
where ``H`` is the Hessian of the average Kullback–Leibler divergence
between the `model` output on a batch of `states` (taken as logits) and
action_logits, with regard to the model parameters.

See [page 3](https://www2.maths.lth.se/matematiklth/vision/publdb/reports/pdf/byrod-eccv-10.pdf)
for more information on the damping coefficient.
"""
function hvp(model, states, action_logits, x; damping_coeff = 1f-2)
    gs = Flux.gradient(Flux.params(model)) do
        gvp(model, states, action_logits, x)
    end
    out = mapreduce(vec, vcat, gs)
    out .+ (damping_coeff .* x)
end

"""
    kld(model, states, action_logits)

Calculate the average Kullback–Leibler divergence
between the `model` and a given `action_logits` on
a batch of `states`.  Numerically stable.
"""
function kld(model, states, action_logits)
    new_action_distribution = model(states)
    map(eachcol(new_action_distribution), eachcol(action_logits)) do a, b
        softmax(a) .* (logsoftmax(a) .- logsoftmax(b)) |> sum
    end |> mean
end

"""
    surrogate_advantage(model, states, actions, advantage, action_logits)
Calculate the surrogate advantage of `model` on experiences (`states` and `actions`)
generated by an old model, which generated `action_logits`. 

See [here](https://spinningup.openai.com/en/latest/algorithms/trpo.html#key-equations) for more information.
"""
function surrogate_advantage(model, states, actions, advantage, action_logits)
    π_θₖ = map(eachcol(softmax(action_logits)), actions) do a, b
        a[b]
    end
    π_θ = map(eachcol(softmax(model(states))), actions) do a, b
        a[b]
    end
    (π_θ ./ π_θₖ) .* advantage |> mean
end

"""
    conjugate_gradient(A, b; max_iter=10, ϵ=1f-10)
Solve ``Ax = b`` approximately with conjugate gradient, with a error tolerance of `ϵ`.

See [Wikipedia](https://en.wikipedia.org/w/index.php?title=Conjugate_gradient_method&oldid=1092048375#Example_code_in_MATLAB_/_GNU_Octave)
for more information.
"""
function conjugate_gradient(A, b; max_iter = 10, ϵ=1f-10)
    x = zeros(Float32, length(b))
    r = b - A(x)
    p = copy(r)
    rsold = r' * r

	iters = min(max_iter, length(b))

    for _ in 1:iters
        Ap = A(p)
        α = rsold / (p' * Ap)
        x .+= α .* p
        r .-= α .* Ap
        rsnew = r' * r
        if sqrt(rsnew) < ϵ
            break
        end
        p .= r .+ (rsnew ./ rsold) .* p
        rsold = rsnew
    end
    x
end
