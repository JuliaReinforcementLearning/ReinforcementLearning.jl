<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLCore · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/rl_core/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" type="text/css"/></head><body><div id="top" class="navbar-wrapper">
<nav class="navbar navbar-expand-lg  navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id="mainNav">
  <div class="container-md">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
  <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
    <span class="navbar-brand">
        <a class="navbar-brand" href="/">
          <!-- <img src="/assets/site/logo.svg" width="30" height="30" alt="logo" loading="lazy"> -->
          JuliaReinforcementLearning
        </a>
    </span>

    <ul class="navbar-nav ml-auto">
        <li class="nav-item">
        <a class="nav-link" href="/get_started/">Get Started</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/guide/">Guide</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contribute/">Contribute</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://github.com/JuliaReinforcementLearning">Github</a>
        </li>
    </ul>
  </div>
</nav>
</div>
<div class="documenter-wrapper" id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rl_base/">RLBase</a></li><li class="is-active"><a class="tocitem" href>RLCore</a></li><li><a class="tocitem" href="../rl_envs/">RLEnvs</a></li><li><a class="tocitem" href="../rl_zoo/">RLZoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLCore</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLCore</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/rl_core.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningCore.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningCore.jl">ReinforcementLearningCore.jl</a><a id="ReinforcementLearningCore.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningCore.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.RLCore" href="#ReinforcementLearningCore.RLCore"><code>ReinforcementLearningCore.RLCore</code></a> — <span class="docstring-category">Module</span></header><section><div><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningCore.jl">ReinforcementLearningCore.jl</a> (<strong>RLCore</strong>) provides some standard and reusable components defined by <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl"><strong>RLBase</strong></a>, hoping that they are useful for people to implement and experiment with different kinds of algorithms.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractApproximator" href="#ReinforcementLearningCore.AbstractApproximator"><code>ReinforcementLearningCore.AbstractApproximator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(app::AbstractApproximator)(env)</code></pre><p>An approximator is a functional object for value estimation. It serves as a black box to provides an abstraction over different  kinds of approximate methods (for example DNN provided by Flux or Knet).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractExplorer" href="#ReinforcementLearningCore.AbstractExplorer"><code>ReinforcementLearningCore.AbstractExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(p::AbstractExplorer)(x)
(p::AbstractExplorer)(x, mask)</code></pre><p>Define how to select an action based on action values.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractHook" href="#ReinforcementLearningCore.AbstractHook"><code>ReinforcementLearningCore.AbstractHook</code></a> — <span class="docstring-category">Type</span></header><section><div><p>A hook is called at different stage duiring a <a href="@ref"><code>run</code></a> to allow users to inject customized runtime logic. By default, a <code>AbstractHook</code> will do nothing. One can override the behavior by implementing the following methods:</p><ul><li><code>(hook::YourHook)(::PreActStage, agent, env, action)</code>, note that there&#39;s an extra argument of <code>action</code>.</li><li><code>(hook::YourHook)(::PostActStage, agent, env)</code></li><li><code>(hook::YourHook)(::PreEpisodeStage, agent, env)</code></li><li><code>(hook::YourHook)(::PostEpisodeStage, agent, env)</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractLearner" href="#ReinforcementLearningCore.AbstractLearner"><code>ReinforcementLearningCore.AbstractLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(learner::AbstractLearner)(env)</code></pre><p>A learner is usually used to estimate state values, state-action values or distributional values based on experiences.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractTrajectory" href="#ReinforcementLearningCore.AbstractTrajectory"><code>ReinforcementLearningCore.AbstractTrajectory</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AbstractTrajectory</code></pre><p>A trajectory is used to record some useful information during the interactions between agents and environments. It behaves similar to a <code>NamedTuple</code> except that we extend it with some optional methods.</p><p>Required Methods:</p><ul><li><code>Base.getindex</code></li><li><code>Base.keys</code></li></ul><p>Optional Methods:</p><ul><li><code>Base.length</code></li><li><code>Base.isempty</code></li><li><code>Base.empty!</code></li><li><code>Base.haskey</code></li><li><code>Base.push!</code></li><li><code>Base.pop!</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ActorCritic" href="#ReinforcementLearningCore.ActorCritic"><code>ReinforcementLearningCore.ActorCritic</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ActorCritic(;actor, critic, optimizer=ADAM())</code></pre><p>The <code>actor</code> part must return logits (<em>Do not use softmax in the last layer!</em>), and the <code>critic</code> part must return a state value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.Agent" href="#ReinforcementLearningCore.Agent"><code>ReinforcementLearningCore.Agent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Agent(;kwargs...)</code></pre><p>A wrapper of an <code>AbstractPolicy</code>. Generally speaking, it does nothing but to update the trajectory and policy appropriately in different stages.</p><p><strong>Keywords &amp; Fields</strong></p><ul><li><code>policy</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a>: the policy to use</li><li><code>trajectory</code>::<a href="#ReinforcementLearningCore.AbstractTrajectory"><code>AbstractTrajectory</code></a>: used to store transitions between an agent and an environment</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.Agent-Tuple{AbstractStage,AbstractEnv}" href="#ReinforcementLearningCore.Agent-Tuple{AbstractStage,AbstractEnv}"><code>ReinforcementLearningCore.Agent</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Here we extend the definition of <code>(p::AbstractPolicy)(::AbstractEnv)</code> in <code>RLBase</code> to accept an <code>AbstractStage</code> as the first argument. Algorithm designers may customize these behaviors respectively by implementing:</p><ul><li><code>(p::YourPolicy)(::AbstractStage, ::AbstractEnv)</code></li><li><code>(p::YourPolicy)(::PreActStage, ::AbstractEnv, action)</code></li></ul><p>The default behaviors for <code>Agent</code> are:</p><ol><li>Update the inner <code>trajectory</code> given the context of <code>policy</code>, <code>env</code>, and <code>stage</code>.</li><li>By default we do nothing.</li><li>In <code>PreActStage</code>, we <code>push!</code> the current <strong>state</strong> and the <strong>action</strong> into   the <code>trajectory</code>.</li><li>In <code>PostActStage</code>, we query the <code>reward</code> and <code>is_terminated</code> info from   <code>env</code> and push them into <code>trajectory</code>.</li><li>For <code>CircularSARTTrajectory</code>:<ol><li>In the <code>PosEpisodeStage</code>, we push the <code>state</code> at the end of an episode and a dummy action into the <code>trajectory</code>.</li><li>In the <code>PreEpisodeStage</code>, we pop out the lastest <code>state</code> and <code>action</code> pair (which are dummy ones) from <code>trajectory</code>.</li></ol></li><li>Update the inner <code>policy</code> given the context of <code>trajectory</code>, <code>env</code>, and <code>stage</code>.</li><li>By default, we only <code>update!</code> the <code>policy</code> in the <code>PreActStage</code>. And it&#39;s   despatched to <code>update!(policy, trajectory)</code>.</li></ol></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.BatchExplorer" href="#ReinforcementLearningCore.BatchExplorer"><code>ReinforcementLearningCore.BatchExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BatchExplorer(explorer::AbstractExplorer)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.BatchExplorer-Tuple{AbstractArray{T,2} where T}" href="#ReinforcementLearningCore.BatchExplorer-Tuple{AbstractArray{T,2} where T}"><code>ReinforcementLearningCore.BatchExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">(x::BatchExplorer)(values::AbstractMatrix)</code></pre><p>Apply inner explorer to each column of <code>values</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}" href="#ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}"><code>ReinforcementLearningCore.BatchStepsPerEpisode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">BatchStepsPerEpisode(batch_size::Int; tag = &quot;TRAINING&quot;)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.StepsPerEpisode"><code>StepsPerEpisode</code></a>, but is specific to environments which return a <code>Vector</code> of rewards (a typical case with <code>MultiThreadEnv</code>).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ComposedHook" href="#ReinforcementLearningCore.ComposedHook"><code>ReinforcementLearningCore.ComposedHook</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ComposedHook(hooks::AbstractHook...)</code></pre><p>Compose different hooks into a single hook.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ComposedStopCondition" href="#ReinforcementLearningCore.ComposedStopCondition"><code>ReinforcementLearningCore.ComposedStopCondition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ComposedStopCondition(stop_conditions...; reducer = any)</code></pre><p>The result of <code>stop_conditions</code> is reduced by <code>reducer</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.DoEveryNEpisode" href="#ReinforcementLearningCore.DoEveryNEpisode"><code>ReinforcementLearningCore.DoEveryNEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DoEveryNEpisode(f; n=1, t=0)</code></pre><p>Execute <code>f(agent, env)</code> every <code>n</code> episode. <code>t</code> is a counter of steps.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.DoEveryNStep" href="#ReinforcementLearningCore.DoEveryNStep"><code>ReinforcementLearningCore.DoEveryNStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DoEveryNStep(f; n=1, t=0)</code></pre><p>Execute <code>f(agent, env)</code> every <code>n</code> step. <code>t</code> is a counter of steps.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EmptyHook" href="#ReinforcementLearningCore.EmptyHook"><code>ReinforcementLearningCore.EmptyHook</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Do nothing</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EpsilonGreedyExplorer" href="#ReinforcementLearningCore.EpsilonGreedyExplorer"><code>ReinforcementLearningCore.EpsilonGreedyExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EpsilonGreedyExplorer{T}(;kwargs...)
EpsilonGreedyExplorer(ϵ) -&gt; EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)</code></pre><blockquote><p>Epsilon-greedy strategy: The best lever is selected for a proportion <code>1 - epsilon</code> of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed_bandit</a></p></blockquote><p>Two kinds of epsilon-decreasing strategy are implmented here (<code>linear</code> and <code>exp</code>).</p><blockquote><p>Epsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed_bandit</a></p></blockquote><p><strong>Keywords</strong></p><ul><li><code>T::Symbol</code>: defines how to calculate the epsilon in the warmup steps. Supported values are <code>linear</code> and <code>exp</code>.</li><li><code>step::Int = 1</code>: record the current step.</li><li><code>ϵ_init::Float64 = 1.0</code>: initial epsilon.</li><li><code>warmup_steps::Int=0</code>: the number of steps to use <code>ϵ_init</code>.</li><li><code>decay_steps::Int=0</code>: the number of steps for epsilon to decay from <code>ϵ_init</code> to <code>ϵ_stable</code>.</li><li><code>ϵ_stable::Float64</code>: the epsilon after <code>warmup_steps + decay_steps</code>.</li><li><code>is_break_tie=false</code>: randomly select an action of the same maximum values if set to <code>true</code>.</li><li><code>rng=Random.GLOBAL_RNG</code>: set the internal RNG.</li><li><code>is_training=true</code>, in training mode, <code>step</code> will not be updated. And the <code>ϵ</code> will be set to 0.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia">s = EpsilonGreedyExplorer{:linear}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)
plot([RL.get_ϵ(s, i) for i in 1:500], label=&quot;linear epsilon&quot;)</code></pre><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/linear_epsilon_greedy_selector.png" alt/></p><pre><code class="language-julia">s = EpsilonGreedyExplorer{:exp}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)
plot([RL.get_ϵ(s, i) for i in 1:500], label=&quot;exp epsilon&quot;)</code></pre><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/exp_epsilon_greedy_selector.png" alt/></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EpsilonGreedyExplorer-Tuple{Any}" href="#ReinforcementLearningCore.EpsilonGreedyExplorer-Tuple{Any}"><code>ReinforcementLearningCore.EpsilonGreedyExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">(s::EpsilonGreedyExplorer)(values; step) where T</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If multiple values with the same maximum value are found. Then a random one will be returned!</p><p><code>NaN</code> will be filtered unless all the values are <code>NaN</code>. In that case, a random one will be returned.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.MultiAgentHook" href="#ReinforcementLearningCore.MultiAgentHook"><code>ReinforcementLearningCore.MultiAgentHook</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultiAgentHook(player=&gt;hook...)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.MultiAgentManager-Tuple" href="#ReinforcementLearningCore.MultiAgentManager-Tuple"><code>ReinforcementLearningCore.MultiAgentManager</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MultiAgentManager(player =&gt; policy...)</code></pre><p>This is the simplest form of multiagent system. At each step they observe the environment from their own perspective and get updated independently. For environments of <code>SEQUENTIAL</code> style, agents which are not the current player will observe a dummy action of <a href="@ref"><code>NO_OP</code></a> in the <code>PreActStage</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.NamedPolicy" href="#ReinforcementLearningCore.NamedPolicy"><code>ReinforcementLearningCore.NamedPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NamedPolicy(name=&gt;policy)</code></pre><p>A policy wrapper to provide a name. Mostly used in multi-agent environments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.NeuralNetworkApproximator" href="#ReinforcementLearningCore.NeuralNetworkApproximator"><code>ReinforcementLearningCore.NeuralNetworkApproximator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NeuralNetworkApproximator(;kwargs)</code></pre><p>Use a DNN model for value estimation.</p><p><strong>Keyword arguments</strong></p><ul><li><code>model</code>, a Flux based DNN model.</li><li><code>optimizer=nothing</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.NoOp" href="#ReinforcementLearningCore.NoOp"><code>ReinforcementLearningCore.NoOp</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Represent no-operation if it&#39;s not the agent&#39;s turn.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.QBasedPolicy" href="#ReinforcementLearningCore.QBasedPolicy"><code>ReinforcementLearningCore.QBasedPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">QBasedPolicy(;learner::Q, explorer::S)</code></pre><p>Use a Q-<code>learner</code> to generate estimations of action values. Then an <code>explorer</code> is applied on the estimations to select an action.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.RandomPolicy" href="#ReinforcementLearningCore.RandomPolicy"><code>ReinforcementLearningCore.RandomPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RandomPolicy(action_space=nothing; rng=Random.GLOBAL_RNG)</code></pre><p>If <code>action_space</code> is <code>nothing</code>, then it will use the <code>legal_action_space</code> at runtime to randomly select an action. Otherwise, a random element within <code>action_space</code> is selected. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You should always set <code>action_space=nothing</code> when dealing with environments of <code>FULL_ACTION_SET</code>.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ResizeImage" href="#ReinforcementLearningCore.ResizeImage"><code>ReinforcementLearningCore.ResizeImage</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ResizeImage(img::Array{T, N})
ResizeImage(dims::Int...) -&gt; ResizeImage(Float32, dims...)
ResizeImage(T::Type{&lt;:Number}, dims::Int...)</code></pre><p>Using BSpline method to resize the <code>state</code> field of an observation to size of <code>img</code> (or <code>dims</code>).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.RewardsPerEpisode" href="#ReinforcementLearningCore.RewardsPerEpisode"><code>ReinforcementLearningCore.RewardsPerEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RewardsPerEpisode(; rewards = Vector{Vector{Float64}}())</code></pre><p>Store each reward of each step in every episode in the field of <code>rewards</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StackFrames" href="#ReinforcementLearningCore.StackFrames"><code>ReinforcementLearningCore.StackFrames</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StackFrames(::Type{T}=Float32, d::Int...)</code></pre><p>Use a pre-initialized <a href="@ref"><code>CircularArrayBuffer</code></a> to store the latest several states specified by <code>d</code>. Before processing any observation, the buffer is filled with `zero{T} by default.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StepsPerEpisode" href="#ReinforcementLearningCore.StepsPerEpisode"><code>ReinforcementLearningCore.StepsPerEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StepsPerEpisode(; steps = Int[], count = 0)</code></pre><p>Store steps of each episode in the field of <code>steps</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterEpisode" href="#ReinforcementLearningCore.StopAfterEpisode"><code>ReinforcementLearningCore.StopAfterEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopAfterEpisode(episode; cur = 0, is_show_progress = true)</code></pre><p>Return <code>true</code> after being called <code>episode</code>. If <code>is_show_progress</code> is <code>true</code>, the <code>ProgressMeter</code> will be used to show progress.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterNoImprovement-Union{Tuple{T}, Tuple{Any,Int64}, Tuple{Any,Int64,T}} where T&lt;:Number" href="#ReinforcementLearningCore.StopAfterNoImprovement-Union{Tuple{T}, Tuple{Any,Int64}, Tuple{Any,Int64,T}} where T&lt;:Number"><code>ReinforcementLearningCore.StopAfterNoImprovement</code></a> — <span class="docstring-category">Method</span></header><section><div><p>StopAfterNoImprovement()</p><p>Stop training when a monitored metric has stopped improving.</p><p>Parameters:</p><p>fn: a closure, return a scalar value, which indicates the performance of the policy (the higher the better) e.g.</p><ol><li>() -&gt; reward(env)</li><li>() -&gt; total<em>reward</em>per_episode.reward</li></ol><p>patience: Number of epochs with no improvement after which training will be stopped.</p><p>δ: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.</p><p>Return <code>true</code> after the monitored metric has stopped improving.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterStep" href="#ReinforcementLearningCore.StopAfterStep"><code>ReinforcementLearningCore.StopAfterStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopAfterStep(step; cur = 1, is_show_progress = true)</code></pre><p>Return <code>true</code> after being called <code>step</code> times.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopSignal" href="#ReinforcementLearningCore.StopSignal"><code>ReinforcementLearningCore.StopSignal</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopSignal()</code></pre><p>Create a stop signal initialized with a value of <code>false</code>. You can manually set it to <code>true</code> by <code>s[] = true</code> to stop the running loop at any time.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopWhenDone" href="#ReinforcementLearningCore.StopWhenDone"><code>ReinforcementLearningCore.StopWhenDone</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopWhenDone()</code></pre><p>Return <code>true</code> if the environment is terminated.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.SumTree" href="#ReinforcementLearningCore.SumTree"><code>ReinforcementLearningCore.SumTree</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SumTree(capacity::Int)</code></pre><p>Efficiently sample and update weights. For more detals, see the post at <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/">here</a>. Here we use a vector to represent the binary tree. Suppose we will have <code>capacity</code> leaves at most. Every time we <code>push!</code> new node into the tree, only the recent <code>capacity</code> node and their sum will be updated! [––––––Parent nodes––––––][––––leaves––––] [size: 2^ceil(Int, log2(capacity))-1 ][     size: capacity   ]</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; t = SumTree(8)
0-element SumTree
julia&gt; for i in 1:16
       push!(t, i)
       end
julia&gt; t
8-element SumTree:
  9.0
 10.0
 11.0
 12.0
 13.0
 14.0
 15.0
 16.0
julia&gt; sample(t)
(2, 10.0)
julia&gt; sample(t)
(1, 9.0)
julia&gt; inds, ps = sample(t,100000)
([8, 4, 8, 1, 5, 2, 2, 7, 6, 6  …  1, 1, 7, 1, 6, 1, 5, 7, 2, 7], [16.0, 12.0, 16.0, 9.0, 13.0, 10.0, 10.0, 15.0, 14.0, 14.0  …  9.0, 9.0, 15.0, 9.0, 14.0, 9.0, 13.0, 15.0, 10.0, 15.0])
julia&gt; countmap(inds)
Dict{Int64,Int64} with 8 entries:
  7 =&gt; 14991
  4 =&gt; 12019
  2 =&gt; 10003
  3 =&gt; 11027
  5 =&gt; 12971
  8 =&gt; 16052
  6 =&gt; 13952
  1 =&gt; 8985
julia&gt; countmap(ps)
Dict{Float64,Int64} with 8 entries:
  9.0  =&gt; 8985
  13.0 =&gt; 12971
  10.0 =&gt; 10003
  14.0 =&gt; 13952
  16.0 =&gt; 16052
  11.0 =&gt; 11027
  15.0 =&gt; 14991
  12.0 =&gt; 12019</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TabularApproximator" href="#ReinforcementLearningCore.TabularApproximator"><code>ReinforcementLearningCore.TabularApproximator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TabularApproximator(table&lt;:AbstractArray, opt)</code></pre><p>For <code>table</code> of 1-d, it will serve as a state value approximator. For <code>table</code> of 2-d, it will serve as a state-action value approximator.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>For <code>table</code> of 2-d, the first dimension is action and the second dimension is state.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TabularRandomPolicy" href="#ReinforcementLearningCore.TabularRandomPolicy"><code>ReinforcementLearningCore.TabularRandomPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TabularRandomPolicy(;table=Dict{Int, Float32}(), rng=Random.GLOBAL_RNG)</code></pre><p>Use a <code>Dict</code> to store action distribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TimePerStep" href="#ReinforcementLearningCore.TimePerStep"><code>ReinforcementLearningCore.TimePerStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TimePerStep(;max_steps=100)
TimePerStep(times::CircularArrayBuffer{Float64}, t::UInt64)</code></pre><p>Store time cost of the latest <code>max_steps</code> in the <code>times</code> field.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TotalBatchRewardPerEpisode-Tuple{Int64}" href="#ReinforcementLearningCore.TotalBatchRewardPerEpisode-Tuple{Int64}"><code>ReinforcementLearningCore.TotalBatchRewardPerEpisode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TotalBatchRewardPerEpisode(batch_size::Int)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.TotalRewardPerEpisode"><code>TotalRewardPerEpisode</code></a>, but is specific to environments which return a <code>Vector</code> of rewards (a typical case with <code>MultiThreadEnv</code>).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TotalRewardPerEpisode" href="#ReinforcementLearningCore.TotalRewardPerEpisode"><code>ReinforcementLearningCore.TotalRewardPerEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TotalRewardPerEpisode(; rewards = Float64[], reward = 0.0)</code></pre><p>Store the total rewards of each episode in the field of <code>rewards</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.Trajectory" href="#ReinforcementLearningCore.Trajectory"><code>ReinforcementLearningCore.Trajectory</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Trajectory(;[trace_name=trace_container]...)</code></pre><p>A simple wrapper of <code>NamedTuple</code>. Define our own type here to avoid type piracy with <code>NamedTuple</code></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.UCBExplorer-Tuple{Any}" href="#ReinforcementLearningCore.UCBExplorer-Tuple{Any}"><code>ReinforcementLearningCore.UCBExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">UCBExplorer(na; c=2.0, ϵ=1e-10, step=1, seed=nothing)</code></pre><p><strong>Arguments</strong></p><ul><li><code>na</code> is the number of actions used to create a internal counter.</li><li><code>t</code> is used to store current time step.</li><li><code>c</code> is used to control the degree of exploration.</li><li><code>seed</code>, set the seed of inner RNG.</li><li><code>is_training=true</code>, in training mode, time step and counter will not be updated.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.UploadTrajectoryEveryNStep" href="#ReinforcementLearningCore.UploadTrajectoryEveryNStep"><code>ReinforcementLearningCore.UploadTrajectoryEveryNStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UploadTrajectoryEveryNStep(;mailbox, n, sealer=deepcopy)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L2">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.VBasedPolicy" href="#ReinforcementLearningCore.VBasedPolicy"><code>ReinforcementLearningCore.VBasedPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">VBasedPolicy(;learner, mapping=default_value_action_mapping)</code></pre><p>The <code>learner</code> must be a value learner. The <code>mapping</code> is a function which returns an action given <code>env</code> and the <code>learner</code>. By default we iterate through all the valid actions and select the best one which lead to the maximum state value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.WeightedExplorer" href="#ReinforcementLearningCore.WeightedExplorer"><code>ReinforcementLearningCore.WeightedExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">WeightedExplorer(;is_normalized::Bool, rng=Random.GLOBAL_RNG)</code></pre><p><code>is_normalized</code> is used to indicate if the feeded action values are alrady normalized to have a sum of <code>1.0</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Elements are assumed to be <code>&gt;=0</code>.</p></div></div><p>See also: <a href="@ref"><code>WeightedSoftmaxExplorer</code></a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.push!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffers.CircularArrayBuffer{T,N},StackFrames{T,N}}} where N where T" href="#Base.push!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffers.CircularArrayBuffer{T,N},StackFrames{T,N}}} where N where T"><code>Base.push!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>When pushing a <code>StackFrames</code> into a <code>CircularArrayBuffer</code> of the same dimension, only the latest frame is pushed. If the <code>StackFrames</code> is one dimension lower, then it is treated as a general <code>AbstractArray</code> and is pushed in as a frame.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDA.device-Tuple{Any}" href="#CUDA.device-Tuple{Any}"><code>CUDA.device</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">device(model)</code></pre><p>Detect the suitable running device for the <code>model</code>. Return <code>Val(:cpu)</code> by default.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.priority-Tuple{AbstractLearner,Any}" href="#ReinforcementLearningBase.priority-Tuple{AbstractLearner,Any}"><code>ReinforcementLearningBase.priority</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_priority(p::AbstractLearner, experience)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{AbstractExplorer,Any,Any}" href="#ReinforcementLearningBase.prob-Tuple{AbstractExplorer,Any,Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">prob(p::AbstractExplorer, x, mask)</code></pre><p>Similart to <code>prob(p::AbstractExplorer, x)</code>, but here only the <code>mask</code>ed elements are considered.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{AbstractExplorer,Any}" href="#ReinforcementLearningBase.prob-Tuple{AbstractExplorer,Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">prob(p::AbstractExplorer, x) -&gt; AbstractDistribution</code></pre><p>Get the action distribution given action values.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{EpsilonGreedyExplorer{var&quot;#s93&quot;,true,R} where R where var&quot;#s93&quot;,Any}" href="#ReinforcementLearningBase.prob-Tuple{EpsilonGreedyExplorer{var&quot;#s93&quot;,true,R} where R where var&quot;#s93&quot;,Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">prob(s::EpsilonGreedyExplorer, values) -&gt;Categorical
prob(s::EpsilonGreedyExplorer, values, mask) -&gt;Categorical</code></pre><p>Return the probability of selecting each action given the estimated <code>values</code> of each action.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{AbstractApproximator,Any}" href="#ReinforcementLearningBase.update!-Tuple{AbstractApproximator,Any}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update!(a::AbstractApproximator, correction)</code></pre><p>Usually the <code>correction</code> is the gradient of inner parameters.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{TabularRandomPolicy,Pair}" href="#ReinforcementLearningBase.update!-Tuple{TabularRandomPolicy,Pair}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update!(p::TabularRandomPolicy, state =&gt; value)</code></pre><p>You should manually check <code>value</code> sum to <code>1.0</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ApproximatorStyle-Tuple{AbstractApproximator}" href="#ReinforcementLearningCore.ApproximatorStyle-Tuple{AbstractApproximator}"><code>ReinforcementLearningCore.ApproximatorStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Used to detect what an <a href="#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a> is approximating.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore._discount_rewards!-Tuple{Any,Any,Any,Any,Nothing}" href="#ReinforcementLearningCore._discount_rewards!-Tuple{Any,Any,Any,Any,Nothing}"><code>ReinforcementLearningCore._discount_rewards!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>assuming rewards and new_rewards are Vector</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6,Any}" href="#ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6,Any}"><code>ReinforcementLearningCore._generalized_advantage_estimation!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>assuming rewards and advantages are Vector</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.check-Tuple{Any,Any}" href="#ReinforcementLearningCore.check-Tuple{Any,Any}"><code>ReinforcementLearningCore.check</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Inject some customized checkings here by overwriting this function</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.consecutive_view-Tuple{AbstractArray,Array{Int64,1}}" href="#ReinforcementLearningCore.consecutive_view-Tuple{AbstractArray,Array{Int64,1}}"><code>ReinforcementLearningCore.consecutive_view</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">consecutive_view(x::AbstractArray, inds; n_stack = nothing, n_horizon = nothing)</code></pre><p>By default, it behaves the same with <code>select_last_dim(x, inds)</code>. If <code>n_stack</code> is set to an int, then for each frame specified by <code>inds</code>, the previous <code>n_stack</code> frames (including the current one) are concatenated as a new dimension. If <code>n_horizon</code> is set to an int, then for each frame specified by <code>inds</code>, the next <code>n_horizon</code> frames (including the current one) are concatenated as a new dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; x = collect(1:5)
5-element Array{Int64,1}:
 1
 2
 3
 4
 5

julia&gt; consecutive_view(x, [2,4])  # just the same with `select_last_dim(x, [2,4])`
2-element view(::Array{Int64,1}, [2, 4]) with eltype Int64:
 2
 4

julia&gt; consecutive_view(x, [2,4];n_stack = 2)
2×2 view(::Array{Int64,1}, [1 3; 2 4]) with eltype Int64:
 1  3
 2  4

julia&gt; consecutive_view(x, [2,4];n_horizon = 2)
2×2 view(::Array{Int64,1}, [2 4; 3 5]) with eltype Int64:
 2  4
 3  5

julia&gt; consecutive_view(x, [2,4];n_horizon = 2, n_stack=2)  # note the order here, first we stack, then we apply the horizon
2×2×2 view(::Array{Int64,1}, [1 2; 2 3]

[3 4; 4 5]) with eltype Int64:
[:, :, 1] =
 1  2
 2  3

[:, :, 2] =
 3  4
 4  5</code></pre><p>See also <a href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/">Frame Skipping and Preprocessing for Deep Q networks</a> to gain a better understanding of state stacking and n-step learning.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T}} where T&lt;:Number" href="#ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T}} where T&lt;:Number"><code>ReinforcementLearningCore.discount_rewards</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">discount_rewards(rewards::VectorOrMatrix, γ::Number;kwargs...)</code></pre><p>Calculate the gain started from the current step with discount rate of <code>γ</code>. <code>rewards</code> can be a matrix.</p><p><strong>Keyword argments</strong></p><ul><li><code>dims=:</code>, if <code>rewards</code> is a <code>Matrix</code>, then <code>dims</code> can only be <code>1</code> or <code>2</code>.</li><li><code>terminal=nothing</code>, specify if each reward follows by a terminal. <code>nothing</code> means the game is not terminated yet. If <code>terminal</code> is provided, then the size must be the same with <code>rewards</code>.</li><li><code>init=nothing</code>, <code>init</code> can be used to provide the the reward estimation of the last state.</li></ul><p><strong>Example</strong></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}" href="#ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}"><code>ReinforcementLearningCore.flatten_batch</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">flatten_batch(x::AbstractArray)</code></pre><p>Merge the last two dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; x = reshape(1:12, 2, 2, 3)
2×2×3 reshape(::UnitRange{Int64}, 2, 2, 3) with eltype Int64:
[:, :, 1] =
 1  3
 2  4

[:, :, 2] =
 5  7
 6  8

[:, :, 3] =
  9  11
 10  12

julia&gt; flatten_batch(x)
2×6 reshape(::UnitRange{Int64}, 2, 6) with eltype Int64:
 1  3  5  7   9  11
 2  4  6  8  10  12</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T,T}} where T&lt;:Number" href="#ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T,T}} where T&lt;:Number"><code>ReinforcementLearningCore.generalized_advantage_estimation</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">generalized_advantage_estimation(rewards::VectorOrMatrix, values::VectorOrMatrix, γ::Number, λ::Number;kwargs...)</code></pre><p>Calculate the generalized advantage estimate started from the current step with discount rate of <code>γ</code> and a lambda for GAE-Lambda of &#39;λ&#39;. <code>rewards</code> and &#39;values&#39; can be a matrix.</p><p><strong>Keyword argments</strong></p><ul><li><code>dims=:</code>, if <code>rewards</code> is a <code>Matrix</code>, then <code>dims</code> can only be <code>1</code> or <code>2</code>.</li><li><code>terminal=nothing</code>, specify if each reward follows by a terminal. <code>nothing</code> means the game is not terminated yet. If <code>terminal</code> is provided, then the size must be the same with <code>rewards</code>.</li></ul><p><strong>Example</strong></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.normlogpdf-Tuple{Any,Any,Any}" href="#ReinforcementLearningCore.normlogpdf-Tuple{Any,Any,Any}"><code>ReinforcementLearningCore.normlogpdf</code></a> — <span class="docstring-category">Method</span></header><section><div><p>GPU automatic differentiable version for the logpdf function of normal distributions. Adding an epsilon value to guarantee numeric stability if sigma is exactly zero (e.g. if relu is used in output layer).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsBase.sample-Tuple{AbstractTrajectory,ReinforcementLearningCore.AbstractSampler}" href="#StatsBase.sample-Tuple{AbstractTrajectory,ReinforcementLearningCore.AbstractSampler}"><code>StatsBase.sample</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sample([rng=Random.GLOBAL_RNG], trajectory, sampler, [traces=Val(keys(trajectory))])</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Here we return a copy instead of a view:</p><ol><li>Each sample is independent of the original <code>trajectory</code> so that <code>trajectory</code> can be updated async.</li><li><a href="https://docs.julialang.org/en/v1/manual/performance-tips/#Copying-data-is-not-always-bad">Copy is not always so bad</a>.</li></ol></div></div></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rl_base/">« RLBase</a><a class="docs-footer-nextpage" href="../rl_envs/">RLEnvs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 27 January 2021 05:46">Wednesday 27 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
