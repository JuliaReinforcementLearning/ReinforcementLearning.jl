<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLBase · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/rl_base/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" type="text/css"/></head><body><div id="top" class="navbar-wrapper">
<nav class="navbar navbar-expand-lg  navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id="mainNav">
  <div class="container-md">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
  <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
    <span class="navbar-brand">
        <a class="navbar-brand" href="/">
          <!-- <img src="/assets/site/logo.svg" width="30" height="30" alt="logo" loading="lazy"> -->
          JuliaReinforcementLearning
        </a>
    </span>

    <ul class="navbar-nav ml-auto">
        <li class="nav-item">
        <a class="nav-link" href="/get_started/">Get Started</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/guide/">Guide</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contribute/">Contribute</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://github.com/JuliaReinforcementLearning">Github</a>
        </li>
    </ul>
  </div>
</nav>
</div>
<div class="documenter-wrapper" id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li class="is-active"><a class="tocitem" href>RLBase</a></li><li><a class="tocitem" href="../rl_core/">RLCore</a></li><li><a class="tocitem" href="../rl_envs/">RLEnvs</a></li><li><a class="tocitem" href="../rl_zoo/">RLZoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLBase</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLBase</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/rl_base.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningBase.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningBase.jl">ReinforcementLearningBase.jl</a><a id="ReinforcementLearningBase.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningBase.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.RLBase" href="#ReinforcementLearningBase.RLBase"><code>ReinforcementLearningBase.RLBase</code></a> — <span class="docstring-category">Module</span></header><section><div><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl">ReinforcementLearningBase.jl</a> (<strong>RLBase</strong>) provides some common constants, traits, abstractions and interfaces in developing reinforcement learning algorithms in Julia. </p><p>Basically, we defined the following two main concepts in reinforcement learning:</p><ul><li><a href="#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a></li><li><a href="#ReinforcementLearningBase.AbstractEnv"><code>AbstractEnv</code></a></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.CONSTANT_SUM" href="#ReinforcementLearningBase.CONSTANT_SUM"><code>ReinforcementLearningBase.CONSTANT_SUM</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Rewards of all players sum to a constant</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.DETERMINISTIC" href="#ReinforcementLearningBase.DETERMINISTIC"><code>ReinforcementLearningBase.DETERMINISTIC</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>No chance player in the environment. And the game is fully deterministic.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.EXPLICIT_STOCHASTIC" href="#ReinforcementLearningBase.EXPLICIT_STOCHASTIC"><code>ReinforcementLearningBase.EXPLICIT_STOCHASTIC</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Usually used to describe <a href="https://en.wikipedia.org/wiki/Extensive-form_game">extensive-form game</a>. The environment contains a chance player and the corresponding probability is known. Therefore, <a href="#ReinforcementLearningBase.prob"><code>prob</code></a><code>(env, player=chance_player(env))</code> must be defined.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.FULL_ACTION_SET" href="#ReinforcementLearningBase.FULL_ACTION_SET"><code>ReinforcementLearningBase.FULL_ACTION_SET</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>The action space of the environment may contains illegal actions</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.GENERAL_SUM" href="#ReinforcementLearningBase.GENERAL_SUM"><code>ReinforcementLearningBase.GENERAL_SUM</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Total rewards of all players may be different in each step</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.IDENTICAL_UTILITY" href="#ReinforcementLearningBase.IDENTICAL_UTILITY"><code>ReinforcementLearningBase.IDENTICAL_UTILITY</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Every player gets the same reward</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.IMPERFECT_INFORMATION" href="#ReinforcementLearningBase.IMPERFECT_INFORMATION"><code>ReinforcementLearningBase.IMPERFECT_INFORMATION</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>The inner state of some players&#39; observations may be different</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.MINIMAL_ACTION_SET" href="#ReinforcementLearningBase.MINIMAL_ACTION_SET"><code>ReinforcementLearningBase.MINIMAL_ACTION_SET</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>All actions in the action space of the environment are legal</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.PERFECT_INFORMATION" href="#ReinforcementLearningBase.PERFECT_INFORMATION"><code>ReinforcementLearningBase.PERFECT_INFORMATION</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>All players observe the same state</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.SAMPLED_STOCHASTIC" href="#ReinforcementLearningBase.SAMPLED_STOCHASTIC"><code>ReinforcementLearningBase.SAMPLED_STOCHASTIC</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Environment contains chance player and the probability is unknown. Usually only a dummy action is allowed in this case.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The chance player (<a href="#ReinforcementLearningBase.chance_player-Tuple{AbstractEnv}"><code>chance_player</code></a><code>(env)</code>) must appears in the result of <a href="@ref"><code>players</code></a><code>(env)</code>. The result of <code>action_space(env, chance_player)</code> should only contains one dummy action.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.SEQUENTIAL" href="#ReinforcementLearningBase.SEQUENTIAL"><code>ReinforcementLearningBase.SEQUENTIAL</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Environment with the <a href="#ReinforcementLearningBase.DynamicStyle-Tuple{T} where T&lt;:AbstractEnv"><code>DynamicStyle</code></a> of <code>SEQUENTIAL</code> must takes actions from different players one-by-one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.SIMULTANEOUS" href="#ReinforcementLearningBase.SIMULTANEOUS"><code>ReinforcementLearningBase.SIMULTANEOUS</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Environment with the <a href="#ReinforcementLearningBase.DynamicStyle-Tuple{T} where T&lt;:AbstractEnv"><code>DynamicStyle</code></a> of <code>SIMULTANEOUS</code> must take in actions from some (or all) players at one time</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.STEP_REWARD" href="#ReinforcementLearningBase.STEP_REWARD"><code>ReinforcementLearningBase.STEP_REWARD</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>We can get reward after each step</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.STOCHASTIC" href="#ReinforcementLearningBase.STOCHASTIC"><code>ReinforcementLearningBase.STOCHASTIC</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>No chance player in the environment. And the game is stochastic. To help increase reproducibility, these environments should generally accept a <code>AbstractRNG</code> as a keyword argument. For some third-party environments, at least a <code>seed</code> is exposed in the constructor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.TERMINAL_REWARD" href="#ReinforcementLearningBase.TERMINAL_REWARD"><code>ReinforcementLearningBase.TERMINAL_REWARD</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Only get reward at the end of environment</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.ZERO_SUM" href="#ReinforcementLearningBase.ZERO_SUM"><code>ReinforcementLearningBase.ZERO_SUM</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Rewards of all players sum to 0. A special case of [<code>CONSTANT_SUM</code>].</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.AbstractEnv" href="#ReinforcementLearningBase.AbstractEnv"><code>ReinforcementLearningBase.AbstractEnv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(env::AbstractEnv)(action, player=current_player(env))</code></pre><p>Super type of all reinforcement learning environments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.AbstractEnvironmentModel" href="#ReinforcementLearningBase.AbstractEnvironmentModel"><code>ReinforcementLearningBase.AbstractEnvironmentModel</code></a> — <span class="docstring-category">Type</span></header><section><div><p>TODO:</p><p>Describe how to model a reinforcement learning environment. TODO: need more investigation Ref: https://bair.berkeley.edu/blog/2019/12/12/mbpo/</p><ul><li>Analytic gradient computation</li><li>Sampling-based planning</li><li>Model-based data generation</li><li>Value-equivalence prediction <a href="https://arxiv.org/pdf/2006.16712.pdf">Model-based Reinforcement Learning: A Survey.</a> <a href="https://sites.google.com/view/mbrl-tutorial">Tutorial on Model-Based Methods in Reinforcement Learning</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.AbstractPolicy" href="#ReinforcementLearningBase.AbstractPolicy"><code>ReinforcementLearningBase.AbstractPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(π::AbstractPolicy)(env) -&gt; action</code></pre><p>Policy is the most basic concept in reinforcement learning. Unlike the definition in some other packages, here a policy is defined as a functional object which takes in an environment and returns an action.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>See discussions <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues/86">here</a> if you are wondering why we define the input as <code>AbstractEnv</code> instead of state.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The policy <code>π</code> may change its internal state but it shouldn&#39;t change <code>env</code>. When it&#39;s really necessary, remember to make a copy of <code>env</code> to keep the original <code>env</code> untouched.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.GoalState" href="#ReinforcementLearningBase.GoalState"><code>ReinforcementLearningBase.GoalState</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use it to represent the <a href="http://proceedings.mlr.press/v37/schaul15.pdf">goal state</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.InformationSet" href="#ReinforcementLearningBase.InformationSet"><code>ReinforcementLearningBase.InformationSet</code></a> — <span class="docstring-category">Type</span></header><section><div><p>See the definition of <a href="https://en.wikipedia.org/wiki/Information_set_(game_theory)">information set</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.InternalState" href="#ReinforcementLearningBase.InternalState"><code>ReinforcementLearningBase.InternalState</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use it to represent the internal state.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.MultiAgent-Tuple{Integer}" href="#ReinforcementLearningBase.MultiAgent-Tuple{Integer}"><code>ReinforcementLearningBase.MultiAgent</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MultiAgent(n::Integer) -&gt; MultiAgent{n}()</code></pre><p><code>n</code> must be ≥ 2.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.Observation" href="#ReinforcementLearningBase.Observation"><code>ReinforcementLearningBase.Observation</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Sometimes people from different field talk about the same thing with a different name. Here we set the <code>Observation{Any}()</code> as the default state style in this package.</p><p>See discussions <a href="https://ai.stackexchange.com/questions/5970/what-is-the-difference-between-an-observation-and-a-state-in-reinforcement-learn">here</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.Space" href="#ReinforcementLearningBase.Space"><code>ReinforcementLearningBase.Space</code></a> — <span class="docstring-category">Type</span></header><section><div><p>A wrapper to treat each element as a sub-space which supports:</p><ul><li><code>Base.in</code></li><li><code>Random.rand</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.WorldSpace" href="#ReinforcementLearningBase.WorldSpace"><code>ReinforcementLearningBase.WorldSpace</code></a> — <span class="docstring-category">Type</span></header><section><div><p>In some cases, we may not be interested in the action/state space. One can return <code>WorldSpace()</code> to keep the interface consistent.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.copy-Tuple{AbstractEnv}" href="#Base.copy-Tuple{AbstractEnv}"><code>Base.copy</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Make an independent copy of <code>env</code>, </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>rng (if <code>env</code> has) is also copied!</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Random.seed!-Tuple{AbstractEnv,Any}" href="#Random.seed!-Tuple{AbstractEnv,Any}"><code>Random.seed!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Set the seed of internal rng</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.ActionStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.ActionStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.ActionStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ActionStyle(env::AbstractEnv)</code></pre><p>For environments of discrete actions, specify whether the current state of <code>env</code> contains a full action set or a minimal action set. By default the <a href="#ReinforcementLearningBase.MINIMAL_ACTION_SET"><code>MINIMAL_ACTION_SET</code></a> is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.ChanceStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.ChanceStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.ChanceStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ChanceStyle(env) = DETERMINISTIC</code></pre><p>Specify which role the chance plays in the <code>env</code>. Possible returns are:</p><ul><li><a href="#ReinforcementLearningBase.STOCHASTIC"><code>STOCHASTIC</code></a>. This is the default return.</li><li><a href="#ReinforcementLearningBase.DETERMINISTIC"><code>DETERMINISTIC</code></a></li><li><a href="#ReinforcementLearningBase.EXPLICIT_STOCHASTIC"><code>EXPLICIT_STOCHASTIC</code></a></li><li><a href="#ReinforcementLearningBase.SAMPLED_STOCHASTIC"><code>SAMPLED_STOCHASTIC</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.DefaultStateStyle-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.DefaultStateStyle-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.DefaultStateStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Specify the defalt state style when calling <code>state(env)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L2">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.DynamicStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.DynamicStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.DynamicStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">DynamicStyle(env::AbstractEnv) = SEQUENTIAL</code></pre><p>Only valid in environments with a <a href="#ReinforcementLearningBase.NumAgentStyle-Tuple{T} where T&lt;:AbstractEnv"><code>NumAgentStyle</code></a> of <a href="#ReinforcementLearningBase.MultiAgent-Tuple{Integer}"><code>MultiAgent</code></a>. Determine whether the players can play simultaneously or not. Possible returns are:</p><ul><li><a href="#ReinforcementLearningBase.SEQUENTIAL"><code>SEQUENTIAL</code></a>. This is the default return.</li><li><a href="#ReinforcementLearningBase.SIMULTANEOUS"><code>SIMULTANEOUS</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.InformationStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.InformationStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.InformationStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">InformationStyle(env) = IMPERFECT_INFORMATION</code></pre><p>Distinguish environments between <a href="#ReinforcementLearningBase.PERFECT_INFORMATION"><code>PERFECT_INFORMATION</code></a> and <a href="#ReinforcementLearningBase.IMPERFECT_INFORMATION"><code>IMPERFECT_INFORMATION</code></a>. <a href="#ReinforcementLearningBase.IMPERFECT_INFORMATION"><code>IMPERFECT_INFORMATION</code></a> is returned by default.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.NumAgentStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.NumAgentStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.NumAgentStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">NumAgentStyle(env)</code></pre><p>Number of agents involved in the <code>env</code>. Possible returns are:</p><ul><li><a href="@ref"><code>SINGLE_AGENT</code></a>. This is the default return.</li><li>[<code>MultiAgent</code>][@ref].</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.RewardStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.RewardStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.RewardStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Specify whether we can get reward after each step or only at the end of an game. Possible values are <a href="#ReinforcementLearningBase.STEP_REWARD"><code>STEP_REWARD</code></a> (the default one) or <a href="#ReinforcementLearningBase.TERMINAL_REWARD"><code>TERMINAL_REWARD</code></a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Environments of <a href="#ReinforcementLearningBase.TERMINAL_REWARD"><code>TERMINAL_REWARD</code></a> style can be viewed as a subset of environments of <a href="#ReinforcementLearningBase.STEP_REWARD"><code>STEP_REWARD</code></a> style. For some algorithms, like MCTS, we may have some a more efficient implementation for environments of <a href="#ReinforcementLearningBase.TERMINAL_REWARD"><code>TERMINAL_REWARD</code></a> style.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.StateStyle-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.StateStyle-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.StateStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">StateStyle(env::AbstractEnv)</code></pre><p>Define the possible styles of <code>state(env)</code>. Possible values are:</p><ul><li><a href="#ReinforcementLearningBase.Observation"><code>Observation{T}</code></a>. This is the default return.</li><li><a href="#ReinforcementLearningBase.InternalState"><code>InternalState{T}</code></a></li><li><a href="@ref"><code>Information{T}</code></a></li><li>You can also define your customized state style when necessary.</li></ul><p>Or a tuple contains several of the above ones.</p><p>This is useful for environments which provide more than one kind of state.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.UtilityStyle-Tuple{T} where T&lt;:AbstractEnv" href="#ReinforcementLearningBase.UtilityStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ReinforcementLearningBase.UtilityStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">UtilityStyle(env::AbstractEnv)</code></pre><p>Specify the utility style in multi-agent environments. Possible values are:</p><ul><li><a href="@ref">GENERAL_SUM</a>. The default return.</li><li><a href="@ref">ZERO_SUM</a></li><li><a href="@ref">CONSTANT_SUM</a></li><li><a href="@ref">IDENTICAL_UTILITY</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.action_space" href="#ReinforcementLearningBase.action_space"><code>ReinforcementLearningBase.action_space</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">action_space(env, player=current_player(env))</code></pre><p>Get all available actions from environment. See also: <a href="#ReinforcementLearningBase.legal_action_space"><code>legal_action_space</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.chance_player-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.chance_player-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.chance_player</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chance_player(env)</code></pre><p>Only valid for environments with a chance player.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.child-Tuple{AbstractEnv,Any}" href="#ReinforcementLearningBase.child-Tuple{AbstractEnv,Any}"><code>ReinforcementLearningBase.child</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">child(env::AbstractEnv, action)</code></pre><p>Treat the <code>env</code> as a game tree. Create an independent child after applying <code>action</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.current_player-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.current_player-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.current_player</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">current_player(env)</code></pre><p>Return the next player to take action. For <a href="https://en.wikipedia.org/wiki/Extensive-form_game">Extensive Form Games</a>, a <em>chance player</em> may be returned. (See also <a href="#ReinforcementLearningBase.chance_player-Tuple{AbstractEnv}"><code>chance_player</code></a>) For <a href="@ref">SIMULTANEOUS</a> environments, a <em>simultaneous player</em> is always returned. (See also <a href="#ReinforcementLearningBase.simultaneous_player-Tuple{Any}"><code>simultaneous_player</code></a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.is_terminated-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.is_terminated-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.is_terminated</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">is_terminated(env, player=current_player(env))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L2">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.legal_action_space" href="#ReinforcementLearningBase.legal_action_space"><code>ReinforcementLearningBase.legal_action_space</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">legal_action_space(env, player=current_player(env))</code></pre><p>For environments of <a href="#ReinforcementLearningBase.MINIMAL_ACTION_SET"><code>MINIMAL_ACTION_SET</code></a>, the result is the same with <a href="#ReinforcementLearningBase.action_space"><code>action_space</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.legal_action_space_mask" href="#ReinforcementLearningBase.legal_action_space_mask"><code>ReinforcementLearningBase.legal_action_space_mask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">legal_action_space_mask(env, player=current_player(env)) -&gt; AbstractArray{Bool}</code></pre><p>Required for environments of <a href="#ReinforcementLearningBase.FULL_ACTION_SET"><code>FULL_ACTION_SET</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.priority-Tuple{AbstractPolicy,Any}" href="#ReinforcementLearningBase.priority-Tuple{AbstractPolicy,Any}"><code>ReinforcementLearningBase.priority</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">priority(π::AbstractPolicy, experience)</code></pre><p>Usually used in offline policies.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.prob" href="#ReinforcementLearningBase.prob"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Get the action distribution of chance player.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Only valid for environments of <a href="#ReinforcementLearningBase.EXPLICIT_STOCHASTIC"><code>EXPLICIT_STOCHASTIC</code></a> style. The current player of <code>env</code> must be the chance player.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{AbstractPolicy,Any,Any}" href="#ReinforcementLearningBase.prob-Tuple{AbstractPolicy,Any,Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">prob(π::AbstractPolicy, env, action)</code></pre><p>Only valid for environments with discrete actions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{AbstractPolicy,Any}" href="#ReinforcementLearningBase.prob-Tuple{AbstractPolicy,Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">prob(π::AbstractPolicy, env) -&gt; Distribution</code></pre><p>Get the probability distribution of actions based on policy <code>π</code> given an <code>env</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.reset!-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.reset!-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.reset!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Reset the internal state of an environment</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.reward" href="#ReinforcementLearningBase.reward"><code>ReinforcementLearningBase.reward</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">reward(env, player=current_player(env))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L2">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.simultaneous_player-Tuple{Any}" href="#ReinforcementLearningBase.simultaneous_player-Tuple{Any}"><code>ReinforcementLearningBase.simultaneous_player</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">simultaneous_player(env)</code></pre><p>Only valid for environments of <a href="#ReinforcementLearningBase.SIMULTANEOUS"><code>SIMULTANEOUS</code></a> style.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.spectator_player-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.spectator_player-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.spectator_player</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spectator_player(env)</code></pre><p>Used in imperfect multi-agent environments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.state-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.state-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.state</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">state(env, style=[DefaultStateStyle(env)], player=[current_player(env)])</code></pre><p>The state can be of any type. However, most neural network based algorithms assume an <code>AbstractArray</code> is returned. For environments with many different states provided (inner state, information state, etc), users need to provide <code>style</code> to declare which kind of state they want.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.state_space-Tuple{AbstractEnv}" href="#ReinforcementLearningBase.state_space-Tuple{AbstractEnv}"><code>ReinforcementLearningBase.state_space</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">state_space(env, style=[DefaultStateStyle(env)], player=[current_player(env)])</code></pre><p>Describe all possible states.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.test_interfaces!-Tuple{Any}" href="#ReinforcementLearningBase.test_interfaces!-Tuple{Any}"><code>ReinforcementLearningBase.test_interfaces!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Call this function after writing your customized environment to make sure that all the necessary interfaces are implemented correctly and consistently.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{AbstractPolicy,Any}" href="#ReinforcementLearningBase.update!-Tuple{AbstractPolicy,Any}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update!(π::AbstractPolicy, experience)</code></pre><p>Update the policy <code>π</code> with online/offline experience or parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.walk-Tuple{Any,AbstractEnv}" href="#ReinforcementLearningBase.walk-Tuple{Any,AbstractEnv}"><code>ReinforcementLearningBase.walk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">walk(f, env::AbstractEnv)</code></pre><p>Call <code>f</code> with <code>env</code> and its descendants. Only use it with small games.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L4">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../rl_core/">RLCore »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 1 February 2021 16:28">Monday 1 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
