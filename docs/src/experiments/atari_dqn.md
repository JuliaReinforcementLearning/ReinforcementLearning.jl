# Playing the Atari Games with DQN

Here we provide an example to show you how to play the Atari games with DQN.
First, let's install some required packages:

- `] add ArcadeLearningEnvironment`. Please note that this package only works on Linux systems. For now, it doesn't use the BinaryBuilder.jl and you have to make sure that some necessary building tools are properly installed(Good First Issue!!!). If you are using Ubuntu, you are lucky. `sudo apt-get install -y --no-install-recommends cmake build-essential libz-dev unzip` should be enough.
- `] add ReinforcementLearningEnvironments`. This package provides some unified interfaces.
- `] add ReinforcementLearning#master`. This package is still under rapid development, so we need to install the master branch.
- `] add CuArrays`. To enable the GPU support.
- `] add Flux#master`. We need the master branch of Flux to use Zygote.jl for backprop.
- `] add Plots`. To plot the rewards.

Now we can try to train a DQN agent.

```julia
using ReinforcementLearning, ReinforcementLearningEnvironments, ArcadeLearningEnvironment, Flux, Plots

using Random
Random.seed!(11)

state_size = (84, 84)
n_frames = 4

env = WrappedEnv(
    env = AtariEnv(;
        name="pong",
        grayscale_obs=true,
        noop_max=30,
        frame_skip=4,
        terminal_on_life_loss=false,
        repeat_action_probability=0.,
        max_num_frames_per_episode=4 * 100000,
        color_averaging=false,
        full_action_space=false,
        seed=(22, 33)
        ),
    preprocessor = Chain(
        ImageResize(84, 84),
        StackFrames(state_size..., n_frames)
    )
)

na = length(action_space(env))

device = :gpu

agent = Agent(
    π = QBasedPolicy(
        learner = DQNLearner(
            approximator = NeuralNetworkQ(
                model = Chain(
                    x -> x ./ 255,
                    Conv((8,8), n_frames => 32, relu; stride=4),
                    Conv((4,4), 32 => 64, relu; stride=2),
                    Conv((3,3), 64 => 64, relu; stride=1),
                    x -> reshape(x, :, size(x)[end]),
                    Dense(7*7*64, 512, relu),
                    Dense(512, na),
                    ),
                optimizer = ADAM(0.00001),
                device = device
            ),
            target_approximator = NeuralNetworkQ(
                model = Chain(
                    x -> x ./ 255,
                    Conv((8,8), n_frames => 32, relu; stride=4),
                    Conv((4,4), 32 => 64, relu; stride=2),
                    Conv((3,3), 64 => 64, relu; stride=1),
                    x -> reshape(x, :, size(x)[end]),
                    Dense(7*7*64, 512, relu),
                    Dense(512, na),
                    ),
                optimizer = ADAM(0.00001),
                device = device
            ),
            update_freq = 4,
            γ = 0.99f0,
            update_horizon = 1,
            batch_size = 32,
            stack_size = n_frames,
            min_replay_history = 10000,
            loss_fun = huber_loss,
            target_update_freq = 1000,
        ),
        selector = EpsilonGreedySelector{:exp}(ϵ_init=1.0, ϵ_stable = 0.01, decay_steps = 30000),
    ),
    buffer = circular_RTSA_buffer(
        capacity = 100000,
        state_eltype = Float32,
        state_size = state_size,
    )
)

hook = ComposedHook(
    TotalRewardPerEpisode(),
    TimePerStep()
);

run(agent, env, StopAfterStep(3000000; is_show_progress=true); hook = hook)
```

Finally we can plot the rewards of each episode:

```
plot(hook[1].rewards, xlabel="Episode", ylabel="Reward", label="")
```

![](../assets/img/pong_dqn.png)