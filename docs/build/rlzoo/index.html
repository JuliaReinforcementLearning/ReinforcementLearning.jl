<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLZoo · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorials/">Tutorials</a></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../tips/">Tips for Developers</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rlbase/">RLBase</a></li><li><a class="tocitem" href="../rlcore/">RLCore</a></li><li><a class="tocitem" href="../rlenvs/">RLEnvs</a></li><li class="is-active"><a class="tocitem" href>RLZoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLZoo</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLZoo</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/master/docs/src/rlzoo.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningZoo.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningZoo.jl">ReinforcementLearningZoo.jl</a><a id="ReinforcementLearningZoo.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningZoo.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.A2CGAELearner" href="#ReinforcementLearningZoo.A2CGAELearner"><code>ReinforcementLearningZoo.A2CGAELearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">A2CGAELearner(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>, an <a href="../rlcore/#ReinforcementLearningCore.ActorCritic"><code>ActorCritic</code></a> based <a href="../rlcore/#ReinforcementLearningCore.NeuralNetworkApproximator"><code>NeuralNetworkApproximator</code></a></li><li><code>γ::Float32</code>, reward discount rate.</li><li><code>λ::Float32</code>, lambda for GAE-lambda</li><li><code>actor_loss_weight::Float32</code></li><li><code>critic_loss_weight::Float32</code></li><li><code>entropy_loss_weight::Float32</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/A2CGAE.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.A2CLearner" href="#ReinforcementLearningZoo.A2CLearner"><code>ReinforcementLearningZoo.A2CLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">A2CLearner(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.ActorCritic"><code>ActorCritic</code></a></li><li><code>γ::Float32</code>, reward discount rate.</li><li><code>actor_loss_weight::Float32</code></li><li><code>critic_loss_weight::Float32</code></li><li><code>entropy_loss_weight::Float32</code></li><li><code>update_freq::Int</code>, usually set to the same with the length of trajectory.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/A2C.jl#L3-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.BasicDQNLearner" href="#ReinforcementLearningZoo.BasicDQNLearner"><code>ReinforcementLearningZoo.BasicDQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BasicDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></p><p>This is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the <code>approximator</code> is usually a <a href="../rlcore/#ReinforcementLearningCore.NeuralNetworkApproximator"><code>NeuralNetworkApproximator</code></a>. You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.</p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>loss_func</code>: the loss function to use.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>rng=Random.GLOBAL_RNG</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/basic_dqn.jl#L3-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.BehaviorCloningPolicy" href="#ReinforcementLearningZoo.BehaviorCloningPolicy"><code>ReinforcementLearningZoo.BehaviorCloningPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BehaviorCloningPolicy(;kw...)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><code>approximator</code>: calculate the logits of possible actions directly</li><li><code>explorer=GreedyExplorer()</code> </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/offline_rl/behavior_cloning.jl#L3-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.BestResponsePolicy-Tuple{Any, Any, Any}" href="#ReinforcementLearningZoo.BestResponsePolicy-Tuple{Any, Any, Any}"><code>ReinforcementLearningZoo.BestResponsePolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">BestResponsePolicy(policy, env, best_responder)</code></pre><ul><li><code>policy</code>, the original policy to be wrapped in the best response policy.</li><li><code>env</code>, the environment to handle.</li><li><code>best_responder</code>, the player to choose best response action.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/best_response_policy.jl#L11-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DDPGPolicy-Tuple{}" href="#ReinforcementLearningZoo.DDPGPolicy-Tuple{}"><code>ReinforcementLearningZoo.DDPGPolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">DDPGPolicy(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>behavior_actor</code>,</li><li><code>behavior_critic</code>,</li><li><code>target_actor</code>,</li><li><code>target_critic</code>,</li><li><code>start_policy</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>ρ = 0.995f0</code>,</li><li><code>batch_size = 32</code>,</li><li><code>start_steps = 10000</code>,</li><li><code>update_after = 1000</code>,</li><li><code>update_every = 50</code>,</li><li><code>act_limit = 1.0</code>,</li><li><code>act_noise = 0.1</code>,</li><li><code>step = 0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/ddpg.jl#L46-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DQNLearner-Union{Tuple{}, Tuple{Tf}, Tuple{Tt}, Tuple{Tq}} where {Tq, Tt, Tf}" href="#ReinforcementLearningZoo.DQNLearner-Union{Tuple{}, Tuple{Tf}, Tuple{Tt}, Tuple{Tq}} where {Tq, Tt, Tf}"><code>ReinforcementLearningZoo.DQNLearner</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">DQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>traces = SARTS</code>: set to <code>SLARTSL</code> if you are to apply to an environment of <code>FULL_ACTION_SET</code>.</li><li><code>rng = Random.GLOBAL_RNG</code></li><li><code>is_enable_double_DQN = Bool</code>: enable double dqn, enabled by default.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/dqn.jl#L23-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DeepCFR" href="#ReinforcementLearningZoo.DeepCFR"><code>ReinforcementLearningZoo.DeepCFR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DeepCFR(;kwargs...)</code></pre><p>Symbols used here follow the paper: <a href="https://arxiv.org/abs/1811.00164">Deep Counterfactual Regret Minimization</a></p><p><strong>Keyword arguments</strong></p><ul><li><code>K</code>, number of traverrsal.</li><li><code>t</code>, number of iteration.</li><li><code>Π</code>, the policy network.</li><li><code>V</code>, a dictionary of each player&#39;s advantage network.</li><li><code>MΠ</code>, a strategy memory.</li><li><code>MV</code>, a dictionary of each player&#39;s advantage memory.</li><li><code>reinitialize_freq=1</code>, the frequency of reinitializing the value networks.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/deep_cfr.jl#L4-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DoubleLearner" href="#ReinforcementLearningZoo.DoubleLearner"><code>ReinforcementLearningZoo.DoubleLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DoubleLearner(;L1, L2, rng=Random.GLOBAL_RNG)</code></pre><p>This is a meta-learner, it will randomly select one learner and update another learner. The estimation of an observation is the sum of result from two learners.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/double_learner.jl#L5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DuelingNetwork" href="#ReinforcementLearningZoo.DuelingNetwork"><code>ReinforcementLearningZoo.DuelingNetwork</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DuelingNetwork(;base, val, adv)</code></pre><p>Dueling network automatically produces separate estimates of the state value function network and advantage function network. The expected output size of val is 1, and adv is the size of the action space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/common.jl#L41-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.EnrichedAction" href="#ReinforcementLearningZoo.EnrichedAction"><code>ReinforcementLearningZoo.EnrichedAction</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EnrichedAction(action;kwargs...)</code></pre><p>Inject some runtime info into the action</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/patch.jl#L5-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.ExperienceBasedSamplingModel" href="#ReinforcementLearningZoo.ExperienceBasedSamplingModel"><code>ReinforcementLearningZoo.ExperienceBasedSamplingModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ExperienceBasedSamplingModel</code></pre><p>Randomly generate a transition of (s, a, r, t, s′) based on previous experiences in each sampling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/dyna_agents/env_models/experience_based_sampling_model.jl#L6-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy" href="#ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy"><code>ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ExternalSamplingMCCFRPolicy</code></pre><p>This implementation uses stochasticaly-weighted averaging.</p><p>Ref:</p><ul><li><a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">MONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES</a></li><li><a href="https://papers.nips.cc/paper/3713-monte-carlo-sampling-for-regret-minimization-in-extensive-games.pdf">Monte Carlo Sampling for Regret Minimization in Extensive Games</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/external_sampling_mccfr.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.GaussianNetwork" href="#ReinforcementLearningZoo.GaussianNetwork"><code>ReinforcementLearningZoo.GaussianNetwork</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GaussianNetwork(;pre=identity, μ, logσ)</code></pre><p>Returns <code>μ</code> and <code>logσ</code> when called.  Create a distribution to sample from  using <code>Normal.(μ, exp.(logσ))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/vpg.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.IQNLearner" href="#ReinforcementLearningZoo.IQNLearner"><code>ReinforcementLearningZoo.IQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">IQNLearner(;kwargs)</code></pre><p>See <a href="https://arxiv.org/abs/1806.06923">paper</a></p><p><strong>Keyworkd arugments</strong></p><ul><li><code>approximator</code>, a <a href="#ReinforcementLearningZoo.ImplicitQuantileNet"><code>ImplicitQuantileNet</code></a></li><li><code>target_approximator</code>, a <a href="#ReinforcementLearningZoo.ImplicitQuantileNet"><code>ImplicitQuantileNet</code></a>, must have the same structure as <code>approximator</code></li><li><code>κ = 1.0f0</code>,</li><li><code>N = 32</code>,</li><li><code>N′ = 32</code>,</li><li><code>Nₑₘ = 64</code>,</li><li><code>K = 32</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>stack_size = 4</code>,</li><li><code>batch_size = 32</code>,</li><li><code>update_horizon = 1</code>,</li><li><code>min_replay_history = 20000</code>,</li><li><code>update_freq = 4</code>,</li><li><code>target_update_freq = 8000</code>,</li><li><code>update_step = 0</code>,</li><li><code>default_priority = 1.0f2</code>,</li><li><code>β_priority = 0.5f0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li><li><code>device_seed = nothing</code>,</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/iqn.jl#L35-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.ImplicitQuantileNet" href="#ReinforcementLearningZoo.ImplicitQuantileNet"><code>ReinforcementLearningZoo.ImplicitQuantileNet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ImplicitQuantileNet(;ψ, ϕ, header)</code></pre><pre><code class="language-none">        quantiles (n_action, n_quantiles, batch_size)
           ↑
         header
           ↑
feature ↱  ⨀   ↰ transformed embedding
       ψ       ϕ
       ↑       ↑
       s        τ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/iqn.jl#L3-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.MinimaxPolicy" href="#ReinforcementLearningZoo.MinimaxPolicy"><code>ReinforcementLearningZoo.MinimaxPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MinimaxPolicy(;value_function, depth::Int)</code></pre><p>The minimax algorithm with <a href="https://en.wikipedia.org/wiki/Alpha-beta_pruning">Alpha-beta pruning</a></p><p><strong>Keyword Arguments</strong></p><ul><li><code>maximum_depth::Int=30</code>, the maximum depth of search.</li><li><code>value_function=nothing</code>, estimate the value of <code>env</code>. <code>value_function(env) -&gt; Number</code>. It is only called after searching for <code>maximum_depth</code> and the <code>env</code> is not terminated yet.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/searching/minimax.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.MonteCarloLearner" href="#ReinforcementLearningZoo.MonteCarloLearner"><code>ReinforcementLearningZoo.MonteCarloLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MonteCarloLearner(;kwargs...)</code></pre><p>Use monte carlo method to estimate state value or state-action value.</p><p><strong>Fields</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.TabularApproximator"><code>TabularApproximator</code></a>, can be either <code>TabularVApproximator</code> or <code>TabularQApproximator</code>.</li><li><code>γ=1.0</code>, discount rate.</li><li><code>kind=FIRST_VISIT</code>. Optional values are <code>FIRST_VISIT</code> or <code>EVERY_VISIT</code>.</li><li><code>sampling=NO_SAMPLING</code>. Optional values are <code>NO_SAMPLING</code>, <code>WEIGHTED_IMPORTANCE_SAMPLING</code> or <code>ORDINARY_IMPORTANCE_SAMPLING</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/monte_carlo_learner.jl#L24-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.MultiThreadEnv" href="#ReinforcementLearningZoo.MultiThreadEnv"><code>ReinforcementLearningZoo.MultiThreadEnv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultiThreadEnv(envs::Vector{&lt;:AbstractEnv})</code></pre><p>Wrap multiple instances of the same environment type into one environment. Each environment will run in parallel by leveraging <code>Threads.@spawn</code>. So remember to set the environment variable <code>JULIA_NUM_THREADS</code>!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/multi_thread_env.jl#L5-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.MultiThreadEnv-Tuple{Any, Int64}" href="#ReinforcementLearningZoo.MultiThreadEnv-Tuple{Any, Int64}"><code>ReinforcementLearningZoo.MultiThreadEnv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MultiThreadEnv(f, n::Int)</code></pre><p><code>f</code> is a lambda function which creates an <code>AbstractEnv</code> by calling <code>f()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/multi_thread_env.jl#L26-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy" href="#ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy"><code>ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OutcomeSamplingMCCFRPolicy</code></pre><p>This implementation uses stochasticaly-weighted averaging.</p><p>Ref:</p><ul><li><a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">MONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES</a></li><li><a href="https://papers.nips.cc/paper/3713-monte-carlo-sampling-for-regret-minimization-in-extensive-games.pdf">Monte Carlo Sampling for Regret Minimization in Extensive Games</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/outcome_sampling_mccfr.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PPOPolicy" href="#ReinforcementLearningZoo.PPOPolicy"><code>ReinforcementLearningZoo.PPOPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PPOPolicy(;kwargs)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>λ = 0.95f0</code>,</li><li><code>clip_range = 0.2f0</code>,</li><li><code>max_grad_norm = 0.5f0</code>,</li><li><code>n_microbatches = 4</code>,</li><li><code>n_epochs = 4</code>,</li><li><code>actor_loss_weight = 1.0f0</code>,</li><li><code>critic_loss_weight = 0.5f0</code>,</li><li><code>entropy_loss_weight = 0.01f0</code>,</li><li><code>dist = Categorical</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul><p>By default, <code>dist</code> is set to <code>Categorical</code>, which means it will only works on environments of discrete actions. To work with environments of continuous actions <code>dist</code> should be set to <code>Normal</code> and the <code>actor</code> in the <code>approximator</code> should be a <code>GaussianNetwork</code>. Using it with a <code>GaussianNetwork</code> supports  multi-dimensional action spaces, though it only supports it under the assumption that the dimensions are independent since the <code>GaussianNetwork</code> outputs a single <code>μ</code> and <code>σ</code> for each dimension which is used to simplify the calculations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/ppo.jl#L55-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PrioritizedDQNLearner" href="#ReinforcementLearningZoo.PrioritizedDQNLearner"><code>ReinforcementLearningZoo.PrioritizedDQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PrioritizedDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a> And also https://danieltakeshi.github.io/2019/07/14/per/</p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li><li><code>rng = Random.GLOBAL_RNG</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/prioritized_dqn.jl#L3-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PrioritizedDQNLearner-Tuple{Any}" href="#ReinforcementLearningZoo.PrioritizedDQNLearner-Tuple{Any}"><code>ReinforcementLearningZoo.PrioritizedDQNLearner</code></a> — <span class="docstring-category">Method</span></header><section><div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The state of the observation is assumed to have been stacked, if <code>!isnothing(stack_size)</code>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/prioritized_dqn.jl#L93-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PrioritizedSweepingSamplingModel" href="#ReinforcementLearningZoo.PrioritizedSweepingSamplingModel"><code>ReinforcementLearningZoo.PrioritizedSweepingSamplingModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PrioritizedSweepingSamplingModel(θ::Float64=1e-4)</code></pre><p>See more details at Section (8.4) on Page 168 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/dyna_agents/env_models/prioritized_sweeping_sampling_model.jl#L7-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.QRDQNLearner-Tuple{}" href="#ReinforcementLearningZoo.QRDQNLearner-Tuple{}"><code>ReinforcementLearningZoo.QRDQNLearner</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">QRDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/pdf/1710.10044.pdf">Distributional Reinforcement Learning with Quantile Regression</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get quantile-values of a batch of states. The output should be of size <code>(n_quantile, n_action)</code>.</li><li><code>target_approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the quantile values of the next state batch.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=1</code>: the frequency of updating the <code>approximator</code>.</li><li><code>n_quantile::Int=1</code>: the number of quantiles.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>traces = SARTS</code>, set to <code>SLARTSL</code> if you are to apply to an environment of <code>FULL_ACTION_SET</code>.</li><li><code>loss_func</code>=<a href="@ref"><code>quantile_huber_loss</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/qr_dqn.jl#L31-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.REMDQNLearner-Union{Tuple{}, Tuple{Tf}, Tuple{Tt}, Tuple{Tq}} where {Tq, Tt, Tf}" href="#ReinforcementLearningZoo.REMDQNLearner-Union{Tuple{}, Tuple{Tf}, Tuple{Tt}, Tuple{Tq}} where {Tq, Tt, Tf}"><code>ReinforcementLearningZoo.REMDQNLearner</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">REMDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1907.04543">An Optimistic Perspective on Offline Reinforcement Learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>ensemble_num::Int=1</code>: the number of ensemble approximators.</li><li><code>ensemble_method::Symbol=:rand</code>: the method of combining Q values. &#39;:rand&#39; represents random ensemble mixture, and &#39;:mean&#39; is the average.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>traces = SARTS</code>, set to <code>SLARTSL</code> if you are to apply to an environment of <code>FULL_ACTION_SET</code>.</li><li><code>rng = Random.GLOBAL_RNG</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/rem_dqn.jl#L24-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.RainbowLearner" href="#ReinforcementLearningZoo.RainbowLearner"><code>ReinforcementLearningZoo.RainbowLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RainbowLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function. It is recommended to use Flux.Losses.logitcrossentropy. Flux.Losses.crossentropy will encounter the problem of negative numbers.</li><li><code>Vₘₐₓ::Float32</code>: the maximum value of distribution.</li><li><code>Vₘᵢₙ::Float32</code>: the minimum value of distribution.</li><li><code>n_actions::Int</code>: number of possible actions.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=500</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float32=1.0f2.</code>: the default priority for newly added transitions. It must be <code>&gt;= 1</code>.</li><li><code>n_atoms::Int=51</code>: the number of buckets of the value function distribution.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>rng = Random.GLOBAL_RNG</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/dqns/rainbow.jl#L3-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.SACPolicy-Tuple{}" href="#ReinforcementLearningZoo.SACPolicy-Tuple{}"><code>ReinforcementLearningZoo.SACPolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">SACPolicy(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>policy</code>,</li><li><code>qnetwork1</code>,</li><li><code>qnetwork2</code>,</li><li><code>target_qnetwork1</code>,</li><li><code>target_qnetwork2</code>,</li><li><code>start_policy</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>ρ = 0.995f0</code>,</li><li><code>α = 0.2f0</code>,</li><li><code>batch_size = 32</code>,</li><li><code>start_steps = 10000</code>,</li><li><code>update_after = 1000</code>,</li><li><code>update_every = 50</code>,</li><li><code>step = 0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul><p><code>policy</code> is expected to output a tuple <code>(μ, logσ)</code> of mean and log standard deviations for the desired action distributions, this can be implemented using a <code>GaussianNetwork</code> in a <code>NeuralNetworkApproximator</code>.</p><p>Implemented based on http://arxiv.org/abs/1812.05905</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/sac.jl#L31-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.TD3Policy-Tuple{}" href="#ReinforcementLearningZoo.TD3Policy-Tuple{}"><code>ReinforcementLearningZoo.TD3Policy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TD3Policy(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>behavior_actor</code>,</li><li><code>behavior_critic</code>,</li><li><code>target_actor</code>,</li><li><code>target_critic</code>,</li><li><code>start_policy</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>ρ = 0.995f0</code>,</li><li><code>batch_size = 32</code>,</li><li><code>start_steps = 10000</code>,</li><li><code>update_after = 1000</code>,</li><li><code>update_every = 50</code>,</li><li><code>policy_freq = 2</code> # frequency in which the actor performs a gradient step and critic target is updated</li><li><code>target_act_limit = 1.0</code>, # noise added to actor target</li><li><code>target_act_noise = 0.1</code>, # noise added to actor target</li><li><code>act_limit = 1.0</code>, # noise added when outputing action</li><li><code>act_noise = 0.1</code>, # noise added when outputing action</li><li><code>step = 0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/td3.jl#L43-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.TabularCFRPolicy-Tuple{}" href="#ReinforcementLearningZoo.TabularCFRPolicy-Tuple{}"><code>ReinforcementLearningZoo.TabularCFRPolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TabularCFRPolicy(;kwargs...)</code></pre><p>Some useful papers while implementing this algorithm:</p><ul><li><a href="http://modelai.gettysburg.edu/2013/cfr/cfr.pdf">An Introduction to Counterfactual Regret Minimization</a></li><li><a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">MONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES</a></li><li><a href="https://arxiv.org/pdf/1407.5042.pdf">Solving Large Imperfect Information Games Using CFR⁺</a></li><li><a href="https://arxiv.org/pdf/1810.11542v1.pdf">Revisiting CFR⁺ and Alternating Updates</a></li><li><a href="https://arxiv.org/pdf/1809.04040.pdf">Solving Imperfect-Information Games via Discounted Regret Minimization</a></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>is_alternating_update=true</code>: If <code>true</code>, we update the players alternatively.</li><li><code>is_reset_neg_regrets=true</code>: Whether to use <strong>regret matching⁺</strong>.</li><li><code>is_linear_averaging=true</code></li><li><code>weighted_averaging_delay=0</code>. The averaging delay in number of iterations. Only valid when <code>is_linear_averaging</code> is set to <code>true</code>.</li><li><code>state_type=String</code>, the data type of information set.</li><li><code>rng=Random.GLOBAL_RNG</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/tabular_cfr.jl#L34-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.TimeBasedSamplingModel" href="#ReinforcementLearningZoo.TimeBasedSamplingModel"><code>ReinforcementLearningZoo.TimeBasedSamplingModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TimeBasedSamplingModel(n_actions::Int, κ::Float64 = 1e-4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/dyna_agents/env_models/time_based_sample_model.jl#L6-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.VPGPolicy" href="#ReinforcementLearningZoo.VPGPolicy"><code>ReinforcementLearningZoo.VPGPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Vanilla Policy Gradient</p><p>VPGPolicy(;kwargs)</p><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>,</li><li><code>baseline</code>,</li><li><code>dist</code>, distribution function of the action</li><li><code>γ</code>, discount factor</li><li><code>α_θ</code>, step size of policy parameter</li><li><code>α_w</code>, step size of baseline parameter</li><li><code>batch_size</code>,</li><li><code>rng</code>,</li><li><code>loss</code>,</li><li><code>baseline_loss</code>,</li></ul><p>if the action space is continuous, then the env should transform the action value, (such as using tanh), in order to make sure low ≤ value ≤ high</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/vpg.jl#L23-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{AbstractTrajectory, Union{NamedPolicy{var&quot;#s12&quot;, N} where {var&quot;#s12&quot;&lt;:(VBasedPolicy{var&quot;#s11&quot;, M} where {var&quot;#s11&quot;&lt;:MonteCarloLearner, M}), N}, QBasedPolicy{var&quot;#s13&quot;, E} where {var&quot;#s13&quot;&lt;:MonteCarloLearner, E&lt;:AbstractExplorer}, VBasedPolicy{var&quot;#s14&quot;, M} where {var&quot;#s14&quot;&lt;:MonteCarloLearner, M}}, AbstractEnv, PreEpisodeStage}" href="#ReinforcementLearningBase.update!-Tuple{AbstractTrajectory, Union{NamedPolicy{var&quot;#s12&quot;, N} where {var&quot;#s12&quot;&lt;:(VBasedPolicy{var&quot;#s11&quot;, M} where {var&quot;#s11&quot;&lt;:MonteCarloLearner, M}), N}, QBasedPolicy{var&quot;#s13&quot;, E} where {var&quot;#s13&quot;&lt;:MonteCarloLearner, E&lt;:AbstractExplorer}, VBasedPolicy{var&quot;#s14&quot;, M} where {var&quot;#s14&quot;&lt;:MonteCarloLearner, M}}, AbstractEnv, PreEpisodeStage}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Empty the trajectory at the end of an episode</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/monte_carlo_learner.jl#L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{DeepCFR, AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{DeepCFR, AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/deep_cfr.jl#L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{DeepCFR}" href="#ReinforcementLearningBase.update!-Tuple{DeepCFR}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Update Π (policy network)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/deep_cfr.jl#L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{ExternalSamplingMCCFRPolicy, AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{ExternalSamplingMCCFRPolicy, AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/external_sampling_mccfr.jl#L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{OutcomeSamplingMCCFRPolicy, AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{OutcomeSamplingMCCFRPolicy, AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/outcome_sampling_mccfr.jl#L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy, AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy, AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/tabular_cfr.jl#L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy}" href="#ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Update the <code>behavior_policy</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/tabular_cfr.jl#L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{VBasedPolicy{var&quot;#s19&quot;, M} where {var&quot;#s19&quot;&lt;:MonteCarloLearner, M}, AbstractTrajectory, AbstractEnv, PostEpisodeStage}" href="#ReinforcementLearningBase.update!-Tuple{VBasedPolicy{var&quot;#s19&quot;, M} where {var&quot;#s19&quot;&lt;:MonteCarloLearner, M}, AbstractTrajectory, AbstractEnv, PostEpisodeStage}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Only update at the end of an episode</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/monte_carlo_learner.jl#L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore._run" href="#ReinforcementLearningCore._run"><code>ReinforcementLearningCore._run</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Many policy gradient based algorithms require that the <code>env</code> is a <code>MultiThreadEnv</code> to increase the diversity during training. So the training pipeline is different from the default one in <code>RLCore</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/run.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.cfr!" href="#ReinforcementLearningZoo.cfr!"><code>ReinforcementLearningZoo.cfr!</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Symbol meanings:</p><p>π: reach prob π′: new reach prob π₋ᵢ: opponents&#39; reach prob p: player to update. <code>nothing</code> means simultaneous update. w: weight v: counterfactual value <strong>before weighted by opponent&#39;s reaching probability</strong> V: a vector containing the <code>v</code> after taking each action with current information set. Used to calculate the <strong>regret value</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/tabular_cfr.jl#L108-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.evaluate-Tuple{SACPolicy, Any}" href="#ReinforcementLearningZoo.evaluate-Tuple{SACPolicy, Any}"><code>ReinforcementLearningZoo.evaluate</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This function is compatible with a multidimensional action space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/sac.jl#L118-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.external_sampling!-Tuple{DeepCFR, AbstractEnv, Any}" href="#ReinforcementLearningZoo.external_sampling!-Tuple{DeepCFR, AbstractEnv, Any}"><code>ReinforcementLearningZoo.external_sampling!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>CFR Traversal with External Sampling</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/deep_cfr.jl#L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.masked_regret_matching-Tuple{Any, Any}" href="#ReinforcementLearningZoo.masked_regret_matching-Tuple{Any, Any}"><code>ReinforcementLearningZoo.masked_regret_matching</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This is the specific regret matching method used in DeepCFR</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/deep_cfr.jl#L174">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.policy_evaluation!-Tuple{}" href="#ReinforcementLearningZoo.policy_evaluation!-Tuple{}"><code>ReinforcementLearningZoo.policy_evaluation!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">policy_evaluation!(;V, π, model, γ, θ)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>V</code>, an <a href="../rlcore/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>.</li><li><code>π</code>, an <code>AbstractPolicy</code>.</li><li><code>model</code>, a distribution based environment model(given a state and action pair, return all possible reward, next state, termination info and corresponding probability).</li><li><code>γ::Float64</code>, discount rate.</li><li><code>θ::Float64</code>, threshold to stop evaluation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/tabular/policy_iteration.jl#L5-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.update_advantage_networks-Tuple{Any, Any}" href="#ReinforcementLearningZoo.update_advantage_networks-Tuple{Any, Any}"><code>ReinforcementLearningZoo.update_advantage_networks</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Update advantage network</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/findmyway/ReinforcementLearning.jl/blob/725503637afece0711a385d2dd4e409de85e9fc6/src/ReinforcementLearningZoo/src/algorithms/cfr/deep_cfr.jl#L103">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rlenvs/">« RLEnvs</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 14 May 2021 19:48">Friday 14 May 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
